{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f01361-0dd5-484e-82c4-84994abe0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c041f5aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#from pathlib import Path\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#from importlib import reload\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#from matplotlib import colors as mcl\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#import numpy as np\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#import sys\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#sys.path.insert(1, str(Path.cwd()))\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "#from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "#from importlib import reload\n",
    "#from matplotlib import colors as mcl\n",
    "#import numpy as np\n",
    "#import pyreadr\n",
    "#import umap\n",
    "#import seaborn as sns\n",
    "#import PIL.Image as pim\n",
    "#import pandas as pd\n",
    "#from matplotlib import pyplot as plt\n",
    "#import sys\n",
    "#sys.path.insert(1, str(Path.cwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36296557",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utilsimc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutilsimc\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mimc\u001b[39;00m\n\u001b[1;32m      2\u001b[0m reload(imc)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utilsimc'"
     ]
    }
   ],
   "source": [
    "import utilsimc as imc\n",
    "reload(imc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c1ab1-3d59-4abf-bccd-ae83285fabc0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158345f2-f91a-4a2b-bbd4-c16048d68e8a",
   "metadata": {},
   "source": [
    "## Data Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb7ead-db9d-4fd4-b470-0c45d05d00e3",
   "metadata": {},
   "source": [
    "Obtain list of all data files from different TMAs and slides and run Mesmer-based segmentation (deepcell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc8be23-48f0-4fac-83ab-91d38051d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain list of filepaths to all samples\n",
    "listtxt = imc.get_sample_paths(Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656bd7a-39a9-47a7-b4f8-3d2f51ecb820",
   "metadata": {},
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d4b8ad-03a1-4005-9103-38efcc381719",
   "metadata": {},
   "source": [
    "**(note)**: Original segmentation and phenotyping was performed using an earlier version of Mesmer, and these segmentations are attached together with the raw data. Although updates to the segmentation model are mostly superficial, a re-segmentation of the data with deepcell version 0.11.0 will produce slightly different results to what is used in the remainder of the analysis. *The analysis may be freely & fully run with a complete re-segmentation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cead36-fc74-41e5-b0dd-9052e311ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run cell & nuclear segmentation of all samples\n",
    "# imc.segment_files(listtxt, nuclear_channel=('191Ir', 'DNA'), membrane_channel='HLAABC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a69b8d-3534-4f2c-a577-32ee02d37bb6",
   "metadata": {},
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a32ebaf-9489-4b77-b8c5-15d1cddea839",
   "metadata": {},
   "source": [
    "Next, obtain file paths to cell & nuclear segmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad8c72-6b12-4642-b55c-a79558e16012",
   "metadata": {},
   "outputs": [],
   "source": [
    "listcell = list(txt.parent/'cell'/(txt.stem + '___cellmask.tiff') for txt in listtxt)\n",
    "all(fp.exists() for fp in listcell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee271293-6332-4c4f-8e4a-1c43d6a4c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "listnuc = list(txt.parent/'cell'/(txt.stem + '___nucmask.tiff') for txt in listtxt)\n",
    "all(fp.exists() for fp in listcell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5f906-f867-42f4-85fd-05745a0151b6",
   "metadata": {},
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bdc1e5-ed58-4476-84e7-efe2eff582c3",
   "metadata": {},
   "source": [
    "## Assemble Single-cell Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9966dd2c-7966-4315-a43b-a1eefa261ef4",
   "metadata": {},
   "source": [
    "Use segmentations and raw-data to obtain single-cell expression and nearest-neighbor datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d5b1d-db57-45ee-9658-e3fecd8104b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adfsc, adfnn = imc.get_scdataset(pathlist=listtxt, cell_pathlist=listcell, k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1473100",
   "metadata": {},
   "source": [
    "#### WARNING: SAVE DATA (overwrites frames on disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19265fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save raw single-cell data to an .h5 store\n",
    "# with pd.HDFStore(Path.cwd()/'scdata_raw.h5') as store:\n",
    "#     store['adfsc'] = adfsc\n",
    "#     store['adfnn'] = adfnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf81770",
   "metadata": {},
   "source": [
    "#### WARNING: LOAD DATA (overwrites frames in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b0025-8196-4ee1-8e2f-3de91d035e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load raw single-cell data to an .h5 store\n",
    "# with pd.HDFStore(Path.cwd()/'scdata_raw.h5') as store:\n",
    "#     adfsc = store['adfsc']\n",
    "#     adfnn = store['adfnn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a359eeec-c141-4b4b-8e2f-1d4a7a0b89a4",
   "metadata": {},
   "source": [
    "Next, align panels between TMAs and slides, dropping nuclear markers and those not present in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c067a-5ee8-4041-b748-d04d7be32e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized singel-cell dataset\n",
    "ndfsc = (\n",
    "    adfsc\n",
    "    .drop(columns='191Ir,193Ir,BetaCatenin,GranzymeB,Histone,YAP1,Collagen,DNA,DNA1,Vimentin'.split(','))\n",
    "    .fillna(0)\n",
    "    .eval('HLADR = HLADR + HLADPDQDR')\n",
    "    .eval('TGFb = TGFb + TGFB')\n",
    "    .eval('pNFKb = pNFKb + pNFKb65')\n",
    "    .drop(columns='HLADPDQDR,TGFB,pNFKb65'.split(',')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed917a3-93e7-446d-a023-54099045bee6",
   "metadata": {},
   "source": [
    "Normalize expression ranges across the markers by applying elbow detection to determine a threshold below the max above which outliers will be compressed (using hyperbolic tangent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72175109-ba62-42b3-937a-883c21f0c88a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nthresh = imc.get_elbows(ndfsc, bias=3, plot=False);\n",
    "ndfsc = np.tanh(ndfsc/nthresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd16ddf-ae18-4ee9-b563-9c94372a77df",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824091ca-78b0-4fa1-aa4b-672abbbe6ad1",
   "metadata": {},
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d6fb5-c9a6-4df8-afa1-81433784b233",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441625f7-487d-495f-9763-ce1d7bc9e668",
   "metadata": {},
   "source": [
    "Batch normalization for this experiment is based on the CyCombine method, which requires a panel file, a metadata file, and FCS files for all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52729019-4a6f-4311-8fc7-b3ab4890ff06",
   "metadata": {},
   "source": [
    "To create the panel file, we will obtain the channels and markers using a sample ROI where marker naming matches our aligned set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697524f-1074-47c1-9f1d-1b61ea6da3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "imd = imc.get_imd(listtxt[3], dropmass=False)\n",
    "# Make a panel file from sample ROI matching TMA-merged marker names\n",
    "panel_df = (\n",
    "    pd.Series(imd.keys(),name='channels')\n",
    "    .to_frame()\n",
    "    .eval('Channel = channels.str.extract(\"(\\(.*\\d.*\\))\",expand=False).str.replace(\"(\\(|\\))\",\"\",regex=True)',engine='python')\n",
    "    .eval('Marker = channels.str.replace(\"\\(.*\\d*.*\\)\",\"\",regex=True)',engine='python')\n",
    "    .eval('strjoin = \"_\"',engine='python')\n",
    "    .eval('Channel = Channel + strjoin + Marker',engine='python')\n",
    "    .drop(['channels','strjoin'],axis=1)\n",
    "    .eval('Type=\"\"',engine='python')\n",
    "    .set_index('Channel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502b1e2-2713-4400-bff0-94eb229731c5",
   "metadata": {},
   "source": [
    "At this point, we will create a separate directory structure which will hold all the required inputs for CyCombine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2040b89c-4b37-444b-a93b-7d258e281bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_bnorm = Path.cwd()/'batchnorm' # master batch-norm dir\n",
    "if not path_bnorm.exists():\n",
    "    path_bnorm.mkdir()\n",
    "\n",
    "path_bdata = path_bnorm/'dataset' # dir holding FCS and corrected data\n",
    "path_bmeta = path_bnorm/'dataset meta' # dir holding metadata and figures\n",
    "\n",
    "for p in (path_bdata, path_bmeta):\n",
    "    if not p.exists():\n",
    "        p.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f966f7-78b2-4f89-8cef-30f3c3cd771d",
   "metadata": {},
   "source": [
    "Having the panel frame, we can export to a panel csv file for annotation of the markers (type vs. state). This is a manual step in which, after export, markers are expertly annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9b718-9f07-4971-baed-df28feb59c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only markers present in merged data\n",
    "panel_df[np.in1d(panel_df['Marker'], ndfsc.columns)].to_csv(path_bmeta/'panel_unannotated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ff086-7feb-4bd3-a13b-f028baacb7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-import previously annotated panel file\n",
    "panel_df = pd.read_csv(path_bmeta/'panel.csv', index_col='Channel')\n",
    "panel_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba01debb-14d5-427a-a2ef-4f91425897ab",
   "metadata": {},
   "source": [
    "Next, we will format columns names in the expression dataframe to correspond to the 'Channel\" annotation in the panel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2449497b-e005-4f2e-9017-01435b4d7081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach channel names to markers (standardize column name format)\n",
    "m2ch = panel_df.reset_index().set_index('Marker')['Channel'].to_dict()\n",
    "ndfsc.columns = ndfsc.columns.map(m2ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e9f1bb-4851-4cfa-b51b-13de0ff4d2ef",
   "metadata": {},
   "source": [
    "To export the metadata file required for batch-norm, we will import sample metadata and format as CyCombine expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802cebb-ff24-458e-a8fa-b4fd6827ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sample metadata\n",
    "metadf = pd.read_excel('samples_meta.xlsx',index_col='txt')\n",
    "\n",
    "# Create numeric batch IDs\n",
    "batch2bi = {v:k for k,v in dict(enumerate(metadf['batch'].unique())).items()}\n",
    "\n",
    "# Make metadata file for batch norm\n",
    "# ... must have columns:\n",
    "# ... Filename:str, batch:int, condition:int, Patient_id:str\n",
    "metafile_df = (\n",
    "    metadf\n",
    "    .eval('batch = batch.map(@batch2bi)',engine='python')\n",
    "    .eval('condition = 1',engine='python')\n",
    "    .eval('Filename = \".fcs\"',engine='python')\n",
    "    .eval('Filename = txt + Filename',engine='python')\n",
    "    .rename({'patient':'Patient_id'},axis=1)\n",
    "    .loc[:, 'Filename,batch,condition,Patient_id'.split(',')]\n",
    "    .set_index('Filename'))\n",
    "\n",
    "# Not using any patient-level mods to normalization, so patient\n",
    "# IDs columns will be same as sample name excluding extension\n",
    "metafile_df = metafile_df.eval('Patient_id = Filename.str.strip(\".fcs\")',engine='python')\n",
    "\n",
    "# Export metadata file in batch norm dir\n",
    "metafile_df.to_csv(path_bmeta/'metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7a16a",
   "metadata": {},
   "source": [
    "Finally, export the FCS file set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269514b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export FCS files\n",
    "imc.make_fcs_set(ndfsc, listtxt, export_dir=path_bdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8202c",
   "metadata": {},
   "source": [
    "### RStudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f53acbc",
   "metadata": {},
   "source": [
    "Having all the necessary inputs for [CyCombine](https://www.nature.com/articles/s41467-022-29383-5), we will use RStudio to run the script \"cycombine_batches.R\" inside the \"batchnorm\" directory. This will correct expression between batches and output diagnostic figures and the corrected dataset as an RDS archive, which we will re-import below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f55fdd",
   "metadata": {},
   "source": [
    ".   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cad96f",
   "metadata": {},
   "source": [
    ".   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d62d8",
   "metadata": {},
   "source": [
    ".   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af897187-3f4e-4043-bf4a-77c477dda122",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0e9fe",
   "metadata": {},
   "source": [
    "## Loading Corrected Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd467271",
   "metadata": {},
   "source": [
    "Import corrected RDS file and reformat to convention used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb9c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore naming to marker name only in expression frame\n",
    "if ndfsc.columns[0] in panel_df.index:\n",
    "    ndfsc.columns = panel_df.loc[ndfsc.columns, 'Marker'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253bc674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and align corrected data from RDS\n",
    "rds_corrected = pyreadr.read_r(path_bdata/'corrected.RDS')\n",
    "ndfsc_corrected = rds_corrected[None]\n",
    "ndfsc_corrected = ndfsc_corrected.rename({'sample':'txt'},axis=1).astype({'label':int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4219115",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndfsc_corrected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c3b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign cell IDs for each sample to reconstruct corrected expression frame\n",
    "\n",
    "if 'label' in ndfsc.index.names:\n",
    "    ndfsc = ndfsc.droplevel('label') # avoid clashes if re-running notebook parts\n",
    "    \n",
    "temp = list()\n",
    "for txt, tdf in ndfsc_corrected.groupby('txt'):\n",
    "    temp.append(\n",
    "        tdf\n",
    "        .set_index(ndfsc.loc[txt].index)\n",
    "        .reset_index()\n",
    "        .set_index(['txt','ci','label'])\n",
    "        .loc[:, ndfsc.columns])\n",
    "ndfsc_corrected = pd.concat(temp,axis=0)\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndfsc_corrected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c02a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach SOM labels to uncorrected\n",
    "if 'label' in ndfsc.index.names:\n",
    "    ndfsc = ndfsc.droplevel('label')\n",
    "\n",
    "ndfsc = ndfsc.join(ndfsc_corrected.reset_index('label')['label'],on=['txt','ci']).set_index('label',append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b801edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndfsc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d7f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any mismatches in corrected data \n",
    "all([\n",
    "    ndfsc.shape == ndfsc_corrected.shape,\n",
    "    all(ndfsc.columns==ndfsc_corrected.columns),\n",
    "    all(ndfsc.index==ndfsc_corrected.index)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cfd686",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndfsc_corrected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc865888",
   "metadata": {},
   "source": [
    "### Visualize correction on a data sample using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a pseudo-balanced per batch sample from uncorrected data\n",
    "uncorr_sample = (\n",
    "    ndfsc\n",
    "    .join(metadf['batch'])\n",
    "    .groupby('batch')\n",
    "    .apply(lambda dfi: dfi.sample(1000,replace=False,random_state=846).sort_index()))\n",
    "uncorr_sample = uncorr_sample.loc[:, ndfsc.columns]\n",
    "\n",
    "# Get corresponding corrected sample\n",
    "corr_sample = ndfsc_corrected.loc[uncorr_sample.droplevel(0).index]\n",
    "corr_sample.set_index(uncorr_sample.index,inplace=True)\n",
    "\n",
    "# Train a UMAP model on the uncorrected sample\n",
    "umodel = umap.UMAP(n_neighbors=30,n_components=2,metric='cosine',min_dist=.25,random_state=492,verbose=False)\n",
    "umodel.fit(uncorr_sample)\n",
    "\n",
    "# Obtain transformed datasets\n",
    "ux_uncorr = pd.DataFrame(umodel.embedding_,columns=['ux','uy'],index=uncorr_sample.index)\n",
    "ux_corr = umodel.transform(corr_sample)\n",
    "ux_corr = pd.DataFrame(ux_corr,columns=['ux','uy'],index=corr_sample.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a110cf",
   "metadata": {},
   "source": [
    "#### Uncorrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(8, 8))\n",
    "for batch_id, g in ux_uncorr.groupby('batch'):\n",
    "    ax.scatter(*g.to_numpy().T, s=1, label=batch_id, zorder=2)\n",
    "ax.grid(alpha=.3, zorder=0)\n",
    "ax.set_title('UMAP of batch samples\\nUncorrected', weight='bold', fontsize=14);\n",
    "ax.legend(markerscale=7, prop=dict(size=7, weight='bold'), ncol=2);\n",
    "ax.set_xlabel('UMAP-y', weight='bold', fontsize=12)\n",
    "ax.set_ylabel('UMAP-x', weight='bold', fontsize=12);\n",
    "f.savefig(path_bmeta/'batches_umap_uncorrected.pdf', transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b342a98",
   "metadata": {},
   "source": [
    "#### Corrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d696f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(8, 8))\n",
    "for batch_id, g in ux_corr.groupby('batch'):\n",
    "    ax.scatter(*g.to_numpy().T, s=1, label=batch_id, zorder=2)\n",
    "ax.grid(alpha=.3, zorder=0)\n",
    "ax.set_title('UMAP of batch samples\\nCorrected', weight='bold', fontsize=14);\n",
    "ax.legend(markerscale=7, prop=dict(size=7, weight='bold'), ncol=2);\n",
    "ax.set_xlabel('UMAP-y', weight='bold', fontsize=12)\n",
    "ax.set_ylabel('UMAP-x', weight='bold', fontsize=12);\n",
    "f.savefig(path_bmeta/'corrected'/'batches_umap_corrected.pdf', transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a3ce5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a181652",
   "metadata": {},
   "source": [
    "## Data Clustering and Phenotyping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99db661c",
   "metadata": {},
   "source": [
    "Run a two-stage clustering using hierarchical clustering within samples (to a high-resolution set number of clusters) and Leiden clustering of the resulting h-clust nodes. Use clustering to phenotype cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a888a8d",
   "metadata": {},
   "source": [
    "### Two-stage clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de16a24e",
   "metadata": {},
   "source": [
    "Define the phenotyping markers, run h-clust on each sample (txt file), and then use Leiden to cluster h-nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8cd650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define phenotyping marker set\n",
    "mset_pheno = 'CD20,CD5,CCND1,SOX11,CD45,CD3,CD4,CD8,FOXP3,CD14,CD11b,CD68,CD163,CD11c,CD31'.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e367eb7",
   "metadata": {},
   "source": [
    "#### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e90010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a phenotyping slice of data (only pheno marker expression)\n",
    "data = ndfsc_corrected[mset_pheno]\n",
    "\n",
    "# Perform hierarchical clustering on each sample individually\n",
    "clustered = Parallel(n_jobs=36)(\n",
    "    delayed(imc.hclust)(\n",
    "        df=tdf,\n",
    "        t=128)\n",
    "    for txt, tdf\n",
    "    in data.groupby('txt',sort=False))\n",
    "tcm = pd.Index(np.concatenate([pd.Series(ci2hlabel).to_numpy() for _, ci2hlabel in clustered]),name='tcm')\n",
    "\n",
    "# Attach within-sample h-clust labels to all cells\n",
    "if 'tcm' in data.index.names:\n",
    "    data.reset_index('tcm',drop=True,inplace=True)\n",
    "data.set_index(tcm,append=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de3aba",
   "metadata": {},
   "source": [
    "#### Leiden clustering & heatmap visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374897fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform leiden clustering of within-sample h-clust\n",
    "# nodes using their mean expression\n",
    "tcmexp = data.groupby(['txt','tcm']).mean()\n",
    "cm,_ = imc.leiden_cluster(\n",
    "    tcmexp,\n",
    "    k=20,\n",
    "    resolution=2.)\n",
    "cmexp = tcmexp.groupby(cm).mean()\n",
    "\n",
    "# Display a heatmap showing expression of resulting\n",
    "# clusters across phenotyping markers, standardized\n",
    "# by columns (each marker's range is normalized to [0, 1])\n",
    "cmexp/= cmexp.max()\n",
    "snx, _ = imc.auto_clustermap(\n",
    "    cmexp,\n",
    "    cbar_pos=(1.05,.1,.075,.025)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d1bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second plot with values standardized across rows in order to aid phenotypic calling.\n",
    "snx2, _ = imc.auto_clustermap(\n",
    "    cmexp.loc[snx.data2d.index],\n",
    "    row_cluster=False,\n",
    "    standard_scale=0,\n",
    "    cbar_pos=(1.05,.1,.075,.025)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb851d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach main cluster labels to cells in 'data'\n",
    "if 'cm' in data.index.names:\n",
    "    data = data.droplevel(level='cm')\n",
    "\n",
    "data = (\n",
    "    data\n",
    "    .join(pd.Series(cm,index=tcmexp.index,name='cm'),on=['txt','tcm'])\n",
    "    .set_index('cm',append=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767568c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to full expression frame\n",
    "\n",
    "for labelname in ('tcm', 'cm'):\n",
    "    if labelname in ndfsc_corrected.index.names:\n",
    "        ndfsc_corrected = ndfsc_corrected.droplevel(labelname, axis=0)\n",
    "        \n",
    "ndfsc_corrected = ndfsc_corrected.join(data[[]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndfsc_corrected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897a8da",
   "metadata": {},
   "source": [
    "#### Phenotyping\n",
    "\n",
    "Phenotyping is done manually by having expert review of expression patterns and counts from data above. The annotation used for the rest of the analysis is stored in the \"cluster_phenotype_calling.xlsx\", which is imported and attached to the working expression frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0418ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in cluster labels\n",
    "cm2label = (\n",
    "    pd.read_excel(\n",
    "        Path.cwd()/'cluster_phenotype_calling.xlsx',\n",
    "        sheet_name='grouping_cm')\n",
    "    .set_index('cm')\n",
    "    .squeeze()\n",
    "    .to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7128e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pheno labels if present (avoids redundancies\n",
    "# if cell is executed multiple times)\n",
    "if 'pheno' in ndfsc_corrected.index.names:\n",
    "    ndfsc_corrected = ndfsc_corrected.droplevel('pheno', axis=0)\n",
    "\n",
    "# Attach cluster labels\n",
    "ndfsc_corrected = ndfsc_corrected.eval(\n",
    "    'pheno = cm.map(@cm2label)',\n",
    "    engine='python'\n",
    ").set_index('pheno',append=True)\n",
    "\n",
    "# Remove cells deemed to be artifacts based on expression and expert review\n",
    "ndfsc_corrected = ndfsc_corrected.query('pheno!=\"artifact\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159973d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndfsc_corrected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9869a8",
   "metadata": {},
   "source": [
    "### Color assignment and pheno-naming convention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f19a92",
   "metadata": {},
   "source": [
    "We are going to assign the phenotypes individual colors, and standardize the naming of clusters. These annotations are contained in the file \"cluster_phenotype_colors.xlsx\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4026ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenocc_custom = (\n",
    "    pd.read_excel(\n",
    "        Path.cwd()/'cluster_phenotype_colors.xlsx',\n",
    "        index_col='Cells')\n",
    "    .rename_axis('pheno')\n",
    "    .rename({'Color':'phenocc'},axis=1))\n",
    "\n",
    "pheno_rename_dict = phenocc_custom['ndfsc_type']\n",
    "pheno_rename_dict = pheno_rename_dict.reset_index().set_index('ndfsc_type')['pheno'].to_dict()\n",
    "\n",
    "phenocc_custom = (\n",
    "    pd.DataFrame((\n",
    "        phenocc_custom['phenocc']\n",
    "        .str\n",
    "        .strip()\n",
    "        .apply(mcl.to_rgb) # Convert hex codes to RGB values\n",
    "        .to_list()),\n",
    "        index=phenocc_custom.index,\n",
    "        columns=list('RGB')))\n",
    "\n",
    "phenocc_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a57cc67",
   "metadata": {},
   "source": [
    "Next, we will carry over new pheno naming to expression frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndfsc_corrected = (\n",
    "    ndfsc_corrected\n",
    "    .eval(\n",
    "        'pheno=pheno.map(@pheno_rename_dict)',\n",
    "        engine='python')\n",
    "    .reset_index('pheno',drop=True)\n",
    "    .set_index('pheno',append=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e964fb47",
   "metadata": {},
   "source": [
    "Finally, we can visualize the resulting heatmap with cluster annotations and assigned colors. Note that numeric clusters which were annotated as the same phenotype will be merged (implicitly metaclustered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccf1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mset_heat = 'CCND1,SOX11,CD11b,CD11c,CD14,CD163,CD68,CD20,CD5,CD3,CD4,CD8,FOXP3,CD31'.split(',')\n",
    "snxdat = ndfsc_corrected.groupby('pheno').mean()\n",
    "snxdat/= snxdat.max()\n",
    "\n",
    "snx,_ = imc.auto_clustermap(\n",
    "    snxdat[mset_heat],\n",
    "    row_colors=phenocc_custom,\n",
    "    standard_scale=None,\n",
    "    vmax=1.,\n",
    "    vmin=0.,\n",
    "    cbar_pos=(.8,.1,.05,.05),\n",
    "    fontsize=14);\n",
    "\n",
    "snx.figure.savefig(Path.cwd()/'cluster_phenotype_heatmap.pdf',bbox_inches='tight',transparent=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export mean, corrected, normalized expression values for phenotypes\n",
    "ndfsc_corrected.groupby('pheno').mean().to_csv(Path.cwd()/'cluster_phenotype_mean_expression.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34da9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make RGB frame for all cells\n",
    "rgb_ndfsc_corrected = (\n",
    "    phenocc_custom\n",
    "    .loc[ndfsc_corrected.index.get_level_values('pheno')]\n",
    "    .set_index(ndfsc_corrected.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get phenotypic counts & frequencies\n",
    "pheno_counts = imc.get_counts(ndfsc_corrected, 'pheno')\n",
    "pheno_freqs = imc.get_freqs(ndfsc_corrected, 'pheno')\n",
    "pheno_totals = pheno_counts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20765867",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dba0a2a",
   "metadata": {},
   "source": [
    "## Cell-to-phenotype Spatial Interactions\n",
    "\n",
    "In this analysis stage, we will quantify how much individual cells interact with phenotypes in their milieu by way of assigning an additive proximity (interaction) score based on distance from the cell to each cell of a given phenotype within a defined pixel (~ micron) radius. The resulting interaction score of a cell to a phenotype is defined as the sum of proximity scores from that cell to all cells of the phenotype within the interaction radius."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1446517",
   "metadata": {},
   "source": [
    "**(note)**: For purposes of interaction quantification, we are going to use the 'adfnn' (nearest-neighbor) frame, as it contains cell locations. Cells which did not have any expression (before correction) were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndfnn = (\n",
    "    adfnn[adfsc[ndfsc.columns].sum(axis=1) > 0.] # remove cells with null uncorrected expression\n",
    "    .join(\n",
    "        ndfsc_corrected[[]], # add phenotypic labels\n",
    "        how='inner')\n",
    "    .loc[:, ['area', 'im', 'jm']] # select cell area, row & col-pixel centroid location\n",
    ")\n",
    "\n",
    "print(ndfnn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d37805",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78eddc",
   "metadata": {},
   "source": [
    "Ensure both the expression & morpho dataframes have the same set of cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndfsc_corrected = ndfsc_corrected.join(ndfnn[[]], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32debf4f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9287ca1f",
   "metadata": {},
   "source": [
    "Obtain interaction scores for all cells at radii 25 and 50 microns (25 and 50 pixels, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2583cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain interaction scores from all cells to all phenotypes at two different radii\n",
    "adfxx_25 = imc.get_intxarea_dset(ndfnn,listcell,clustering='pheno',rad=25,n_jobs=30,verbose=True)\n",
    "adfxx_50 = imc.get_intxarea_dset(ndfnn,listcell,clustering='pheno',rad=50,n_jobs=30,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d5a386",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d67cb67",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2b1ef6",
   "metadata": {},
   "source": [
    "### Neighborhood profiling - 25um"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd5242",
   "metadata": {},
   "source": [
    "Perform neighborhood analysis (identification of milieus with similar local phenotypic enrichment) with milieu cutoff set at a radius of 25 microns (equivalent to 25 pixels in IMC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parallel hierarchical clustering of samples\n",
    "# *** ignores phenograph metaclustering ***)\n",
    "res, _ = imc.parallel_hclust(adfxx_25/adfxx_25.max(),norm=False,n_jobs=16,seed=492,verbose=True)\n",
    "h_xxcm = res.index.get_level_values('hcm').rename('h_xxcm')\n",
    "\n",
    "# Add sample clustering to interactions frame\n",
    "if 'h_xxcm' in adfxx_25.index.names:\n",
    "    adfxx_25.reset_index('h_xxcm',drop=True,inplace=True)\n",
    "adfxx_25 = adfxx_25.set_index(h_xxcm,append=True)\n",
    "\n",
    "# Run optimized leiden metaclustering\n",
    "xxcm,_ = imc.leiden_cluster(\n",
    "    (adfxx_25/adfxx_25.max()).groupby(['txt', 'h_xxcm']).mean(),\n",
    "    k=50,\n",
    "    seed=9123)\n",
    "\n",
    "# Geta meta-cluster labels for h-clusts\n",
    "xxcm = np.array([f'sp25_{i:02}' for i in xxcm])\n",
    "if 'xxcm' in adfxx_25.index.names:\n",
    "    adfxx_25.reset_index('xxcm', drop=True, inplace=True)\n",
    "tempdata = adfxx_25.groupby(['txt','h_xxcm']).mean()\n",
    "\n",
    "# Add add clustering labels to interactions frame\n",
    "adfxx_25 = (\n",
    "    adfxx_25\n",
    "    .join(\n",
    "        tempdata\n",
    "        .join(pd.Series(xxcm, name='xxcm', index=tempdata.index))['xxcm'],\n",
    "        on=['txt', 'h_xxcm'])\n",
    "    .set_index('xxcm', append=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc2df6b",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd808093",
   "metadata": {},
   "source": [
    "Obtain counts and frequencies of the identified neighborhoods (interaction profile clusters) across samples, as well as total count of neighborhoods (in terms of cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929265ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp25_counts = imc.get_counts(adfxx_25, 'xxcm')\n",
    "sp25_freqs = imc.get_freqs(adfxx_25, 'xxcm')\n",
    "sp25_totals = sp25_counts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e742ecb",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad0d1c",
   "metadata": {},
   "source": [
    "Visualize relative enrichment of phenotypes across the neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45284156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain relative enrichment scores and export file\n",
    "snxdat = adfxx_25.groupby(['xxcm']).mean()\n",
    "snxdat.to_csv(Path.cwd()/'sp25_mean_pheno_enrichment.csv')\n",
    "snxdat/= snxdat.max()\n",
    "\n",
    "# Arrange neighborhood profiles by similarity (dendrogram order)\n",
    "ftemp = sns.clustermap(snxdat,method='ward')\n",
    "snxord = ftemp.data2d.index\n",
    "plt.close(ftemp.figure)\n",
    "\n",
    "# Apply colorization to neighborhoods\n",
    "snxcc = pd.DataFrame(\n",
    "    plt.cm.nipy_spectral(np.linspace(0.1,.95,snxord.size))[:,:3],\n",
    "    index=snxord,\n",
    "    columns=list('RGB')).loc[snxdat.index]\n",
    "\n",
    "# Plot enrichments\n",
    "snx,_ = imc.auto_clustermap(\n",
    "    snxdat,\n",
    "    row_colors=snxcc,\n",
    "    cmap='Reds',\n",
    "    aspect=1.05,\n",
    "    linecolor='w',\n",
    "    linewidth=.5,\n",
    "    cbar_pos=(.875,.35,.1,.025),\n",
    "    fontsize=14\n",
    ");\n",
    "\n",
    "# Add figure title and save result\n",
    "snx.ax_heatmap.set_title('Neighborhoods - 25 um intx raidus',weight='bold',fontsize=14,y=1.05);\n",
    "snx.figure.savefig(Path.cwd()/'sp25_mean_pheno_enrichment.pdf',bbox_inches='tight',transparent=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_adfxx_25 = snxcc.loc[adfxx_25.index.get_level_values('xxcm')].set_index(adfxx_25.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387f946",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc81b4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bc1205",
   "metadata": {},
   "source": [
    "### Neighborhood profiling - 50um"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c048b",
   "metadata": {},
   "source": [
    "Perform neighborhood analysis (identification of milieus with similar local phenotypic enrichment) with milieu cutoff set at a radius of 50 microns (equivalent to 50 pixels in IMC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1853c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parallel hierarchical clustering of samples\n",
    "# *** ignores phenograph metaclustering ***)\n",
    "res, _ = imc.parallel_hclust(adfxx_50/adfxx_50.max(),norm=False,n_jobs=16,seed=492,verbose=True)\n",
    "h_xxcm = res.index.get_level_values('hcm').rename('h_xxcm')\n",
    "\n",
    "# Add sample clustering to interactions frame\n",
    "if 'h_xxcm' in adfxx_50.index.names:\n",
    "    adfxx_50.reset_index('h_xxcm',drop=True,inplace=True)\n",
    "adfxx_50 = adfxx_50.set_index(h_xxcm,append=True)\n",
    "\n",
    "# Run optimized leiden metaclustering\n",
    "xxcm,_ = imc.leiden_cluster(\n",
    "    (adfxx_50/adfxx_50.max()).groupby(['txt', 'h_xxcm']).mean(),\n",
    "    k=50,\n",
    "    seed=9123)\n",
    "\n",
    "# Geta meta-cluster labels for h-clusts\n",
    "xxcm = np.array([f'sp25_{i:02}' for i in xxcm])\n",
    "if 'xxcm' in adfxx_50.index.names:\n",
    "    adfxx_50.reset_index('xxcm', drop=True, inplace=True)\n",
    "tempdata = adfxx_50.groupby(['txt','h_xxcm']).mean()\n",
    "\n",
    "# Add add clustering labels to interactions frame\n",
    "adfxx_50 = (\n",
    "    adfxx_50\n",
    "    .join(\n",
    "        tempdata\n",
    "        .join(pd.Series(xxcm, name='xxcm', index=tempdata.index))['xxcm'],\n",
    "        on=['txt', 'h_xxcm'])\n",
    "    .set_index('xxcm', append=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc40980",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb107afa",
   "metadata": {},
   "source": [
    "Obtain counts and frequencies of the identified neighborhoods (interaction profile clusters) across samples, as well as total count of neighborhoods (in terms of cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171cf8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp50_counts = imc.get_counts(adfxx_50, 'xxcm')\n",
    "sp50_freqs = imc.get_freqs(adfxx_50, 'xxcm')\n",
    "sp50_totals = sp50_counts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166e403",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881cdd10",
   "metadata": {},
   "source": [
    "Visualize relative enrichment of phenotypes across the neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be5f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain relative enrichment scores and export file\n",
    "snxdat = adfxx_50.groupby(['xxcm']).mean()\n",
    "snxdat.to_csv(Path.cwd()/'sp50_mean_pheno_enrichment.csv')\n",
    "snxdat/= snxdat.max()\n",
    "\n",
    "# Arrange neighborhood profiles by similarity (dendrogram order)\n",
    "ftemp = sns.clustermap(snxdat,method='ward')\n",
    "snxord = ftemp.data2d.index\n",
    "plt.close(ftemp.figure)\n",
    "\n",
    "# Apply colorization to neighborhoods\n",
    "snxcc = pd.DataFrame(\n",
    "    plt.cm.nipy_spectral(np.linspace(0.1,.95,snxord.size))[:,:3],\n",
    "    index=snxord,\n",
    "    columns=list('RGB')).loc[snxdat.index]\n",
    "\n",
    "# Plot enrichments\n",
    "snx,_ = imc.auto_clustermap(\n",
    "    snxdat,\n",
    "    row_colors=snxcc,\n",
    "    cmap='Blues',\n",
    "    aspect=1.05,\n",
    "    linecolor='w',\n",
    "    linewidth=.5,\n",
    "    cbar_pos=(.875,.35,.1,.025),\n",
    "    fontsize=14\n",
    ");\n",
    "\n",
    "# Add figure title and save result\n",
    "snx.ax_heatmap.set_title('Neighborhoods - 50 um intx raidus',weight='bold',fontsize=14,y=1.05);\n",
    "snx.figure.savefig(Path.cwd()/'sp50_mean_pheno_enrichment.pdf',bbox_inches='tight',transparent=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_adfxx_50 = snxcc.loc[adfxx_50.index.get_level_values('xxcm')].set_index(adfxx_50.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c747f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16bb662",
   "metadata": {},
   "source": [
    "## Result assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea869b95",
   "metadata": {},
   "source": [
    "### Verify data\n",
    "Check data shapes and index alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8842a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify same cell set across datasets\n",
    "(\n",
    "    ndfsc_corrected.shape,\n",
    "    ndfnn.shape,\n",
    "    adfxx_25.shape,\n",
    "    adfxx_50.shape,\n",
    "    rgb_ndfsc_corrected.shape,\n",
    "    rgb_adfxx_25.shape,\n",
    "    rgb_adfxx_50.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a466e5",
   "metadata": {},
   "source": [
    "### Save all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(Path.cwd()/'datasets.h5') as store:\n",
    "    store['ndfsc_corrected'] = ndfsc_corrected # single-cell expression\n",
    "    store['ndfnn'] = ndfnn # morphology (area, centroid location)\n",
    "    store['adfxx_25'] = adfxx_25 # 25-micron interactions\n",
    "    store['adfxx_50'] = adfxx_50 # 50-micron interactions\n",
    "    store['rgb_ndfsc_corrected'] = rgb_ndfsc_corrected # cell phenotypic colors\n",
    "    store['rgb_adfxx_25'] = rgb_adfxx_25 # cell 25-intx colors\n",
    "    store['rgb_adfxx_50'] = rgb_adfxx_50 # cell 50-intx colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db90e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_counts.to_csv('./pheno_counts.csv')\n",
    "pheno_freqs.to_csv('./pheno_freqs.csv')\n",
    "pheno_totals.to_csv('./pheno_totals.csv')\n",
    "\n",
    "sp25_counts.to_csv('./sp25_counts.csv')\n",
    "sp25_freqs.to_csv('./sp25_freqs.csv')\n",
    "sp25_totals.to_csv('./sp25_totals.csv')\n",
    "\n",
    "sp50_counts.to_csv('./sp50_counts.csv')\n",
    "sp50_freqs.to_csv('./sp50_freqs.csv')\n",
    "sp50_totals.to_csv('./sp50_totals.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ceb487-e66d-4c3d-b8f5-70279a6bd4f5",
   "metadata": {},
   "source": [
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97de63-3bf5-445a-9195-8778c8781ebe",
   "metadata": {},
   "source": [
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84923167-f46d-4427-a91e-b6214be1352a",
   "metadata": {},
   "source": [
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5523a02-8c7c-44a4-8da4-322c343c1cfa",
   "metadata": {},
   "source": [
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abf6f4-7b1f-4f1d-ba28-093c063acb50",
   "metadata": {},
   "source": [
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
